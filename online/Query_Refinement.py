import nltk
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet
from spellchecker import SpellChecker
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
from joblib import load
import spacy


# Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù„Ù joblib ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ tuple (vectorizer, matrix):
   # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ùˆ ÙˆØ­Ø¯Ù‡

#####################################################################################################################################
# corect the query 
nlp = spacy.load('en_core_web_sm')
def correct_spelling(query):
    spell = SpellChecker()
    corrected_query = " ".join([spell.correction(word) if spell.correction(word) is not None else word for word in query.split()])
    return corrected_query
# ================== ğŸ”§ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ==================


# ================== âœ… Ø¯Ø§Ù„Ø© Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­ ==================

def suggest_queries(query, vectorizer, top_n=10):
    doc = nlp(query)
    suggestions = []
    for token in doc:
        if token.text in vectorizer.vocabulary_:
            index = vectorizer.vocabulary_[token.text]
            similar_terms = vectorizer.get_feature_names_out()[index:index+top_n]
            suggestions.extend(similar_terms)
    return suggestions # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±


# ================== ğŸ§ª ØªØ¬Ø±Ø¨Ø© Ù…Ø­Ù„ÙŠØ© ==================



